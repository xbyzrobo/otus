{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "\n",
    "### Д/з из четырех пунктов:\n",
    "* Улучшение `fit_generator`\n",
    "* Сравнение двух ReLU (разные активации)\n",
    "* Испорченный батч-норм \n",
    "* \"Сырые\" данные. \n",
    "\n",
    "### Что нужно сделать\n",
    "* Следовать инструкциям в каждом из пунктов.\n",
    "* Результатами вашей работы будет ноутбук с доработанным кодом + архив с директорией с логами `tensorboard` `logs/`, в который вы запишите результаты экспериментов. Подробности в инструкциях ниже.\n",
    "* Можно и нужно пользоваться кодом из файла `utils`, **но** весь код модифицируйте, пожалуйста, в ноутбуках! Так мне будет проще проверять."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Загрузка tensorboard в ноутбук**\n",
    "\n",
    "Можете попробовать использовать его так на свой страх и риск :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Импорты**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт слоев для д/з"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils2 import BatchNormFlawed, Dense, DenseSmart, Sequential, MNISTSequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных\n",
    "\n",
    "> Здесь ничего менять не нужно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_tr, y_tr), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = MNISTSequence(X_tr, y_tr, 128)\n",
    "test_seq = MNISTSequence(X_test, y_test, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Очистка данных**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf logs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Улучшение fit_generator\n",
    "\n",
    "Улучшите метод `fit_generator` так, чтобы он:\n",
    "* Записывал значения градиентов для всех переменных при помощи `tf.summary.histogram` \n",
    "* Записывал значения ошибки и метрики на валидации с помощью `tf.summary.scalar`\n",
    "\n",
    "Затем сделайте monkey patch класса sequential обновленным методом (следующая ячейка за методом `fit_generator`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_generator(self, train_seq, eval_seq, epoch, loss, optimizer, writer=None):\n",
    "    history = dict(train=list(), val=list())\n",
    "\n",
    "    train_loss_results = list()\n",
    "    val_loss_results = list()\n",
    "\n",
    "    train_accuracy_results = list()\n",
    "    val_accuracy_results = list()\n",
    "\n",
    "    step = 0\n",
    "    val_step = 0\n",
    "    for e in range(epoch):\n",
    "        p = tf.keras.metrics.Mean()\n",
    "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        epoch_loss_avg_val = tf.keras.metrics.Mean()\n",
    "\n",
    "        epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        epoch_accuracy_val = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "        for x, y in train_seq:\n",
    "            with tf.GradientTape() as tape:\n",
    "                \"\"\"\n",
    "                Обратите внимание! Если записывать гистограмму каждый шаг,\n",
    "                обучение будет идти очень медленно. Поэтому записываем данные \n",
    "                каждый i-й шаг.\n",
    "                \"\"\"\n",
    "                if step % 50 == 0:\n",
    "                    prediction = self._forward(x, writer, step)\n",
    "                else:\n",
    "                    prediction = self._forward(x)\n",
    "                loss_value = loss(y, prediction)\n",
    "                    \n",
    "            ###############################################################\n",
    "            #                                                             #\n",
    "            # Добавьте запись градиентов в гистограммы                    #\n",
    "            #                                                             #\n",
    "            ###############################################################\n",
    "            gradients = tape.gradient(loss_value, self._trainable_variables)\n",
    "               \n",
    "            if step % 50 == 0:\n",
    "                \"\"\"\n",
    "                Пример того, как можно дать всем градиентам уникальные имена. \n",
    "                Обратите внимание! Создание grad_names лучше вынести из цикла,\n",
    "                чтобы не пересоздавать список на каждом шаге! \n",
    "                \"\"\"\n",
    "                grad_names = list()\n",
    "                for layer in self._layers:          \n",
    "                    for var_num, var in enumerate(layer.get_trainable()):\n",
    "                        grad_names.append(f\"grad_{layer.name}_{var_num}\")\n",
    "                zipped_gradients = zip(grad_names, gradients)\n",
    "                with writer.as_default():\n",
    "                    for k, v in zipped_gradients:\n",
    "                        tf.summary.histogram(k, v, step=step)\n",
    "                        \n",
    "            optimizer.apply_gradients(zip(gradients, self._trainable_variables))\n",
    "            epoch_accuracy.update_state(y, prediction)\n",
    "            epoch_loss_avg.update_state(loss_value)\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                with writer.as_default():\n",
    "                    tf.summary.scalar('train_accuracy', epoch_accuracy.result().numpy(), step=step)\n",
    "                    tf.summary.scalar('train_loss', epoch_loss_avg.result().numpy(), step=step)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        train_accuracy_results.append(epoch_accuracy.result().numpy())\n",
    "        train_loss_results.append(epoch_loss_avg.result().numpy())\n",
    "\n",
    "        for x, y in eval_seq:\n",
    "            prediction = self._forward(x)\n",
    "            loss_value = loss(y, prediction)\n",
    "            epoch_loss_avg_val.update_state(loss_value)\n",
    "            epoch_accuracy_val.update_state(y, prediction)\n",
    "     \n",
    "            ###############################################################\n",
    "            #                                                             #\n",
    "            # Добавьте сохранение метрики и функции ошибки на валидации   #\n",
    "            #                                                             #\n",
    "            ###############################################################\n",
    "            if val_step % 50 == 0:\n",
    "                with writer.as_default():\n",
    "                    tf.summary.scalar('validation_accuracy', epoch_accuracy_val.result().numpy(), step=val_step)\n",
    "                    tf.summary.scalar('validation_loss', epoch_loss_avg_val.result().numpy(), step=val_step)\n",
    "            val_step += 1\n",
    "            \n",
    "        val_accuracy_results.append(epoch_accuracy_val.result().numpy())\n",
    "        val_loss_results.append(epoch_loss_avg_val.result().numpy())\n",
    "\n",
    "        # print(f\"Epoch train loss: {epoch_train_loss[-1]:.2f},\\nEpoch val loss: {epoch_val_loss[-1]:.2f}\\n{'-'*20}\")\n",
    "        print(\"Epoch {}: Train loss: {:.3f} Train Accuracy: {:.3f}\".format(e + 1,\n",
    "                                                                           train_loss_results[-1],\n",
    "                                                                           train_accuracy_results[-1]))\n",
    "        print(\"Epoch {}: Val loss: {:.3f} Val Accuracy: {:.3f}\".format(e + 1,\n",
    "                                                                       val_loss_results[-1],\n",
    "                                                                       val_accuracy_results[-1]))\n",
    "        print('*' * 20)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monkey patch: обновляем метод\n",
    "Sequential.fit_generator = fit_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Сравнение двух ReLU (разные активации)\n",
    "\n",
    "Запустите два эксперимента ниже. Сравните результаты - значения метрик после каждого из них.\n",
    "\n",
    "Запустите tensorboard, изучите распределения активаций, градиентов и т.д. для `relu` и `smart_dense_relu`. \n",
    "\n",
    "Подумайте, почему в одном случае сеть обучается плохо, а в другом - хорошо. Вставьте в ноутбук (или напишите список названий) тех графики из tensorboard, которые, по вашему мнению, это иллюстрируют, и напишите, почему.\n",
    "\n",
    "\n",
    "Команда для запуска tensorboard в bash:\n",
    "\n",
    "`$ tensorboard --logdir logs/`\n",
    "\n",
    "**Ваш комментарий:**\n",
    "\n",
    "\n",
    "\n",
    "---------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 13.302 Train Accuracy: 0.173\n",
      "Epoch 1: Val loss: 10.411 Val Accuracy: 0.352\n",
      "********************\n",
      "Epoch 2: Train loss: 9.929 Train Accuracy: 0.383\n",
      "Epoch 2: Val loss: 9.109 Val Accuracy: 0.433\n",
      "********************\n",
      "Epoch 3: Train loss: 8.688 Train Accuracy: 0.460\n",
      "Epoch 3: Val loss: 8.427 Val Accuracy: 0.477\n",
      "********************\n",
      "Epoch 4: Train loss: 8.347 Train Accuracy: 0.481\n",
      "Epoch 4: Val loss: 8.276 Val Accuracy: 0.485\n",
      "********************\n",
      "Epoch 5: Train loss: 7.546 Train Accuracy: 0.530\n",
      "Epoch 5: Val loss: 7.052 Val Accuracy: 0.561\n",
      "********************\n",
      "Epoch 6: Train loss: 6.943 Train Accuracy: 0.568\n",
      "Epoch 6: Val loss: 6.742 Val Accuracy: 0.581\n",
      "********************\n",
      "Epoch 7: Train loss: 6.771 Train Accuracy: 0.579\n",
      "Epoch 7: Val loss: 6.707 Val Accuracy: 0.583\n",
      "********************\n",
      "Epoch 8: Train loss: 6.174 Train Accuracy: 0.615\n",
      "Epoch 8: Val loss: 5.420 Val Accuracy: 0.662\n",
      "********************\n",
      "Epoch 9: Train loss: 5.113 Train Accuracy: 0.680\n",
      "Epoch 9: Val loss: 4.837 Val Accuracy: 0.698\n",
      "********************\n",
      "Epoch 10: Train loss: 4.678 Train Accuracy: 0.708\n",
      "Epoch 10: Val loss: 4.552 Val Accuracy: 0.716\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "writer = tf.summary.create_file_writer(\"logs/relu\")\n",
    "\n",
    "model = Sequential(Dense(784, 100, tf.nn.relu, 'dense'), \n",
    "                   Dense(100, 100, tf.nn.relu, 'dense1'), \n",
    "                   Dense(100, 10, tf.nn.softmax, 'dense2'))\n",
    "\n",
    "hist = model.fit_generator(train_seq, test_seq, 10,\n",
    "                           keras.losses.sparse_categorical_crossentropy, \n",
    "                           keras.optimizers.Adam(),\n",
    "                           writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 0.328 Train Accuracy: 0.905\n",
      "Epoch 1: Val loss: 0.153 Val Accuracy: 0.954\n",
      "********************\n",
      "Epoch 2: Train loss: 0.130 Train Accuracy: 0.962\n",
      "Epoch 2: Val loss: 0.126 Val Accuracy: 0.963\n",
      "********************\n",
      "Epoch 3: Train loss: 0.088 Train Accuracy: 0.974\n",
      "Epoch 3: Val loss: 0.107 Val Accuracy: 0.968\n",
      "********************\n",
      "Epoch 4: Train loss: 0.063 Train Accuracy: 0.981\n",
      "Epoch 4: Val loss: 0.104 Val Accuracy: 0.969\n",
      "********************\n",
      "Epoch 5: Train loss: 0.049 Train Accuracy: 0.985\n",
      "Epoch 5: Val loss: 0.105 Val Accuracy: 0.969\n",
      "********************\n",
      "Epoch 6: Train loss: 0.037 Train Accuracy: 0.988\n",
      "Epoch 6: Val loss: 0.107 Val Accuracy: 0.970\n",
      "********************\n",
      "Epoch 7: Train loss: 0.031 Train Accuracy: 0.990\n",
      "Epoch 7: Val loss: 0.114 Val Accuracy: 0.970\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "writer = tf.summary.create_file_writer(\"logs/relu_smart_dense\")\n",
    "\n",
    "model = Sequential(DenseSmart(784, 100, tf.nn.relu, 'dense'), \n",
    "                   DenseSmart(100, 100, tf.nn.relu, 'dense1'), \n",
    "                   DenseSmart(100, 10, tf.nn.softmax, 'dense2'))\n",
    "\n",
    "hist = model.fit_generator(train_seq, test_seq, 10,\n",
    "                           keras.losses.sparse_categorical_crossentropy, \n",
    "                           keras.optimizers.Adam(),\n",
    "                           writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.a Испорченный батч-норм \n",
    "\n",
    "Запустите два эксперимент ниже. \n",
    "\n",
    "Почему обучение не идет? В чем ошибка в слое `BatchNorm`? Изучите и исправьте код метода `__call__` (Шаблон находится ниже под блоком с экспериментом.).\n",
    "\n",
    "Можно пользоваться tensorboard, если он нужен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU + Batch Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.create_file_writer(\"logs/relu_bn\")\n",
    "\n",
    "model = Sequential(Dense(784, 100, tf.nn.relu, 'dense'), \n",
    "                   BatchNormFlawed('batch_norm'), \n",
    "                   Dense(100, 100, tf.nn.relu, 'dense1'), \n",
    "                   Dense(100, 10, tf.nn.softmax, 'dense2'))\n",
    "\n",
    "hist = model.fit_generator(train_seq, test_seq, 10,\n",
    "                           keras.losses.sparse_categorical_crossentropy, \n",
    "                           keras.optimizers.Adam(),\n",
    "                           writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Класс, который нужно исправить**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormFixed(BatchNormFlawed):\n",
    "    def __call__(self, x, writer=None, step=None):\n",
    "        \"\"\"\n",
    "        Исправьте блок кода ниже так, чтобы модель обучалась, не появлялись значения loss = NaN        \"\"\"\n",
    "        mu = tf.reduce_mean(x, axis=0)\n",
    "        sigma = tf.math.reduce_std(x, axis=0)\n",
    "        normed = (x - mu) / sigma \n",
    "        out = normed * self._gamma + self._beta\n",
    "        \"\"\"\n",
    "        Конец блока, который нужно исправить\n",
    "        \"\"\"\n",
    "        \n",
    "        if writer is not None:\n",
    "            with writer.as_default():\n",
    "                tf.summary.histogram(self.name + '_beta', self._beta, step=step)\n",
    "                tf.summary.histogram(self.name + '_gamma', self._gamma, step=step)\n",
    "                tf.summary.histogram(self.name + '_normed', normed, step=step)\n",
    "                tf.summary.histogram(self.name + '_out', out, step=step)\n",
    "                tf.summary.histogram(self.name + '_sigma', sigma, step=step)\n",
    "                tf.summary.histogram(self.name + '_mu', mu, step=step)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b Исправленный батч-норм \n",
    "\n",
    "Запустите эксперимент ниже. \n",
    "\n",
    "Обучается ли сеть? Идет ли процесс обучения лучше, чем в эксперименте с ReLU? \n",
    "\n",
    "Сравните обучение сетей c ReLU и слоем `Dense` (а не `DenseSmart`!) и ReLU с BatchNorm в tensorboard, как в задании 2.\n",
    "Напишите ваши выводы.\n",
    "\n",
    "_Обратите внимание, что слева в интерфейсе tensorboard есть меню, которое позволяет выключать визуализацию ненужных экспериментов._\n",
    "\n",
    "**Ваш комментарий:**\n",
    "\n",
    "\n",
    "\n",
    "---------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.create_file_writer(\"logs/relu_bn_fixed\")\n",
    "\n",
    "model = Sequential(Dense(784, 100, tf.nn.relu, 'dense'), \n",
    "                   BatchNormFixed('batch_norm'), \n",
    "                   Dense(100, 100, tf.nn.relu, 'dense1'), \n",
    "                   Dense(100, 10, tf.nn.softmax, 'dense2'))\n",
    "\n",
    "hist = model.fit_generator(train_seq, test_seq, 10,\n",
    "                           keras.losses.sparse_categorical_crossentropy, \n",
    "                           keras.optimizers.Adam(),\n",
    "                           writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. \"Сырые\" данные. \n",
    "\n",
    "Что будет, если заставить сеть обучаться на сырых данных? \n",
    "\n",
    "Напишите такую функцию `preprocess`, которая не делает min-max scaling изображений и оставляет их в изначальном диапазоне. Не убирайте reshape! Конечно, она должна менять форму матрицы входных данных от `(n x 28 x 28)` к `(n x 784)`. \n",
    "\n",
    "Затем передайте функцию в MNISTSequence, создайте новую train- и test- последовательности запустите эксперимент, используя их как входные данные. \n",
    "\n",
    "Сравните результаты экспериментов c `DenseSmart` + ReLU и обработанными изображениями и `DenseSmart` + ReLU c необработанными изображениями. \n",
    "\n",
    "Обучается ли нейросеть? Если нет, то почему? Сделайте выводы, как в задании 2.\n",
    "\n",
    "**Ваш комментарий:**\n",
    "\n",
    "\n",
    "\n",
    "---------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Шаблон Preprocess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X, y):\n",
    "    \"\"\"\n",
    "    Ваш код\n",
    "    \"\"\"\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создание генераторов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_raw = MNISTSequence(...)\n",
    "test_seq_raw = MNISTSequence(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Эксперимент**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.create_file_writer(\"logs/raw\")\n",
    "\n",
    "model = Sequential(DenseSmart(784, 100, tf.nn.relu, 'dense'), \n",
    "                   DenseSmart(100, 100, tf.nn.relu, 'dense1'), \n",
    "                   DenseSmart(100, 10, tf.nn.softmax, 'dense2'))\n",
    "\n",
    "hist = model.fit_generator(train_seq_raw, test_seq_raw, 10,\n",
    "                           keras.losses.sparse_categorical_crossentropy, \n",
    "                           keras.optimizers.Adam(),\n",
    "                           writer\n",
    "                          )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
